import requests
import concurrent.futures
import os
import pandas as pd
import json
from datetime import datetime, timezone, timedelta
from dotenv import load_dotenv

# ä¸¥æ ¼éµå¾ªä½ æä¾›çš„ SDK å¯¼å…¥æ–¹å¼
from cozepy import Coze, TokenAuth, COZE_CN_BASE_URL

# ================= 1. é…ç½®ä¸ç¯å¢ƒåˆå§‹åŒ– =================
print("ğŸš€ [1/6] æ­£åœ¨åˆå§‹åŒ–ç¯å¢ƒé…ç½®...")
if os.path.exists(".env"):
    load_dotenv()

TOKEN = os.getenv("GITHUB_TOKEN")
FEISHU_WEBHOOK = os.getenv("FEISHU_WEBHOOK")
COZE_API_TOKEN = os.getenv("COZE_API_TOKEN")

# ä½ æŒ‡å®šçš„ Workflow ID
workflow_id = '7600384889276547126'

# æœç´¢å‚æ•°
BASE_QUERY = "skills language:Python created:>2025-10-10 is:public stars:>100"
SPECIFIC_LICENSES = ["mit", "apache-2.0", "gpl-3.0", "0bsd", "cc0-1.0"]

# ç›®å½•ç»“æ„
DIR_TOTAL, DIR_CHANGES, DIR_LOGS = "Data_Total", "Data_Changes", "Logs"
MAJOR_TOTAL_CSV = os.path.join(DIR_TOTAL, "major_licenses_total.csv")
OTHER_TOTAL_CSV = os.path.join(DIR_TOTAL, "other_licenses_total.csv")
LOG_FILE = os.path.join(DIR_LOGS, "update_log.txt")

# ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
for folder in [DIR_TOTAL, DIR_CHANGES, DIR_LOGS]:
    if not os.path.exists(folder):
        os.makedirs(folder, exist_ok=True)

# ================= 2. æ ¸å¿ƒå·¥å…·å‡½æ•° =================

def get_now_bj():
    """è·å–å½“å‰åŒ—äº¬æ—¶é—´"""
    return datetime.now(timezone.utc).astimezone(timezone(timedelta(hours=8))).strftime("%Y-%m-%d %H:%M:%S")

def convert_to_bj_time(utc_str):
    """GitHub UTC æ—¶é—´è½¬åŒ—äº¬æ—¶é—´"""
    if not utc_str: return ""
    try:
        utc_dt = datetime.strptime(utc_str, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.utc)
        return utc_dt.astimezone(timezone(timedelta(hours=8))).strftime("%Y-%m-%d %H:%M:%S")
    except:
        return utc_str

def save_daily_change(df, prefix, label, date_suffix):
    """ä¿å­˜æ¯æ—¥å˜åŠ¨åˆ° Data_Changes æ–‡ä»¶å¤¹"""
    file_name = os.path.join(DIR_CHANGES, f"{prefix}_{label}_{date_suffix}.csv")
    if os.path.exists(file_name):
        existing_df = pd.read_csv(file_name)
        combined_df = pd.concat([existing_df, df]).drop_duplicates('Repo_ID', keep='last')
        combined_df.to_csv(file_name, index=False, encoding='utf-8-sig')
    else:
        df.to_csv(file_name, index=False, encoding='utf-8-sig')
    print(f"    ğŸ’¾ [æ–‡ä»¶å·²ç”Ÿæˆ/æ›´æ–°] {file_name}")

def fetch_github_data(query_suffix):
    """è¯·æ±‚ GitHub API è·å–æ•°æ®"""
    url = "https://api.github.com/search/repositories"
    full_query = f"{BASE_QUERY} {query_suffix}"
    params = {"q": full_query, "sort": "stars", "order": "desc", "per_page": 100}
    headers = {"Accept": "application/vnd.github+json", "User-Agent": "Monitor-Bot"}
    if TOKEN:
        headers["Authorization"] = f"Bearer {TOKEN}"
    try:
        res = requests.get(url, params=params, headers=headers, timeout=20)
        if res.status_code == 200:
            items = res.json().get('items', [])
            print(f"    - æŸ¥è¯¢ [{query_suffix}] æˆåŠŸï¼Œè·å–åˆ° {len(items)} æ¡æ•°æ®")
            return items
        else:
            print(f"    - æŸ¥è¯¢ [{query_suffix}] å¤±è´¥ï¼ŒçŠ¶æ€ç : {res.status_code}")
            return []
    except Exception as e:
        print(f"    - æŸ¥è¯¢ [{query_suffix}] å¼‚å¸¸: {e}")
        return []

# ================= 3. æ‰£å­å·¥ä½œæµ (åŸå°ä¸åŠ¨ä½¿ç”¨ä½ çš„é€»è¾‘) =================

def run_coze_workflow(new_items):
    if not COZE_API_TOKEN or not new_items:
        print("âš ï¸ [Coze] è·³è¿‡è§¦å‘: æœªé…ç½® Token æˆ–æ— æ–°å¢é¡¹ç›®")
        return True

    print(f"ğŸ¤– [Coze] æ­£åœ¨ä½¿ç”¨å®˜æ–¹ SDK è§¦å‘å·¥ä½œæµ...")
    try:
        # --- ä½ çš„åŸç‰ˆä»£ç å¼€å§‹ ---
        coze = Coze(auth=TokenAuth(token=COZE_API_TOKEN), base_url=COZE_CN_BASE_URL)
        repo_list_str = "\n".join([f"- {i['Name']}: {i['URL']}" for i in new_items])
        
        workflow = coze.workflows.runs.create(
            workflow_id=workflow_id,
            parameters={
                "repo_info": repo_list_str
            }
        )
        # --- ä½ çš„åŸç‰ˆä»£ç ç»“æŸ ---

        print("âœ… [Coze] workflow.data:", workflow.data)
        return True
    except Exception as e:
        print(f"âŒ [Coze] è§¦å‘å¤±è´¥: {e}")
        return False

# ================= 4. æ ¸å¿ƒå¢é‡å¤„ç†é€»è¾‘ (è¿™é‡Œä¹‹å‰æ¼äº†ä¿å­˜è°ƒç”¨) =================

def process_incremental(new_list, file_path, label):
    print(f"ğŸ” [å¤„ç†] æ­£åœ¨åˆ†æ {label} åˆ†ç»„æ•°æ®...")
    now_bj = get_now_bj()
    date_suffix = datetime.now().strftime('%m%d')

    new_df = pd.DataFrame([{
        'Repo_ID': i['id'], 'Name': i['full_name'], 'Stars': i['stargazers_count'],
        'License': i['license']['key'] if i['license'] else "None", 'URL': i['html_url'],
        'Created_At': convert_to_bj_time(i['created_at']), 'Updated_At': convert_to_bj_time(i['updated_at']),
        'Last_Grabbed_At': now_bj
    } for i in new_list])

    # 1. é¦–æ¬¡åˆå§‹åŒ–å¤„ç†
    if not os.path.exists(file_path):
        print(f"  - [{label}] é¦–æ¬¡è¿è¡Œï¼Œæ­£åœ¨åˆ›å»ºåˆå§‹æ€»è¡¨...")
        new_df['First_Grabbed_At'] = now_bj
        new_df.to_csv(file_path, index=False, encoding='utf-8-sig')
        return [], 0, len(new_df), [f"[{now_bj}] {label} é¦–æ¬¡åˆå§‹åŒ–ã€‚"]

    old_df = pd.read_csv(file_path)
    old_df['Repo_ID'] = old_df['Repo_ID'].astype(int)

    # 2. è¯†åˆ«æ–°å¢é¡¹ç›®å¹¶ä¿å­˜ (Data_Changes/New_...)
    new_mask = ~new_df['Repo_ID'].isin(old_df['Repo_ID'])
    new_items_df = new_df[new_mask].copy()
    if not new_items_df.empty:
        new_items_df['First_Grabbed_At'] = now_bj
        print(f"  - [{label}] å‘ç° {len(new_items_df)} ä¸ªæ–° Repoï¼Œæ­£åœ¨ä¿å­˜å¢é‡æ–‡ä»¶...")
        save_daily_change(new_items_df, "New", label, date_suffix) # <--- è¿™é‡Œä¹‹å‰è½ä¸‹äº†

    # 3. è¯†åˆ«æŒ‡æ ‡å˜æ›´é¡¹ç›®å¹¶ä¿å­˜ (Data_Changes/Update_...)
    merged = pd.merge(new_df, old_df, on='Repo_ID', suffixes=('_new', '_old'))
    changed_mask = (merged['Stars_new'] != merged['Stars_old']) | (merged['Updated_At_new'] != merged['Updated_At_old'])
    changed_items_raw = merged[changed_mask]
    if not changed_items_raw.empty:
        changed_items_df = new_df[new_df['Repo_ID'].isin(changed_items_raw['Repo_ID'])].copy()
        print(f"  - [{label}] å‘ç° {len(changed_items_df)} ä¸ª Repo æŒ‡æ ‡æœ‰å˜åŠ¨ï¼Œæ­£åœ¨ä¿å­˜å˜æ›´æ–‡ä»¶...")
        save_daily_change(changed_items_df, "Update", label, date_suffix) # <--- è¿™é‡Œä¹‹å‰è½ä¸‹äº†

    # 4. æ›´æ–°æ€»è¡¨ (Data_Total/...)
    first_grabbed_map = old_df.set_index('Repo_ID')['First_Grabbed_At'].to_dict()
    new_df['First_Grabbed_At'] = new_df['Repo_ID'].map(first_grabbed_map).fillna(now_bj)
    updated_total = pd.concat([new_df, old_df]).drop_duplicates('Repo_ID', keep='first')
    updated_total.to_csv(file_path, index=False, encoding='utf-8-sig')

    log_entries = [f"æ–°å¢ï¼š{r['Name']} (â˜…{r['Stars']})" for _, r in new_items_df.iterrows()]
    return new_items_df.to_dict('records'), len(changed_items_raw), len(updated_total), ([f"[{label}]"] + log_entries if log_entries else [])

# ================= 5. é£ä¹¦æ¨é€é€»è¾‘ =================

def send_feishu_v2_card(new_major, new_other, update_count, total_major, total_other, all_logs, coze_success=True):
    if not FEISHU_WEBHOOK:
        print("âš ï¸ [é£ä¹¦] æœªé…ç½® Webhookï¼Œè·³è¿‡æ¨é€")
        return
    
    print("âœ‰ï¸ [é£ä¹¦] æ­£åœ¨æ„å»ºæ¨é€å¡ç‰‡...")
    sync_content = "[å·²åŒæ­¥è‡³é£ä¹¦å¤šç»´è¡¨æ ¼](https://bytedance.larkoffice.com/base/ObLQbDL5QaWfypsafgecLuhRn8f?from=from_copylink)" if coze_success else "âŒ **åŒæ­¥å¤±è´¥ (Coze æµç¨‹é”™è¯¯)**"

    total_new = len(new_major) + len(new_other)
    major_md = "\n".join([f"â€¢ [{i['Name']}]({i['URL']}) <font color='grey'>ğŸ£{i['Created_At'][:10]}</font> **â˜… {i['Stars']}**" for i in new_major[:5]]) or "æš‚æ— æ–°å¢"
    other_md = "\n".join([f"â€¢ [{i['Name']}]({i['URL']}) <font color='grey'>ğŸ£{i['Created_At'][:10]}</font> **â˜… {i['Stars']}**" for i in new_other[:5]]) or "æš‚æ— æ–°å¢"
    log_preview = "\n".join([line.strip() for line in all_logs if line.strip()][:8])

    card_payload = {
        "msg_type": "interactive",
        "card": {
            "body": {
                "direction": "vertical",
                "elements": [
                    {"tag": "column_set", "flex_mode": "stretch", "horizontal_spacing": "12px",
                     "columns": [
                         {"tag": "column", "width": "weighted", "weight": 1, "background_style": "red-50", "padding": "12px",
                          "elements": [{"tag": "markdown", "content": "**<font color='red'>ä¸»æµç»„</font>**"}, {"tag": "markdown", "content": major_md}]},
                         {"tag": "column", "width": "weighted", "weight": 1, "background_style": "orange-50", "padding": "12px",
                          "elements": [{"tag": "markdown", "content": "**<font color='orange'>éä¸»æµç»„</font>**"}, {"tag": "markdown", "content": other_md}]}
                     ]},
                    {"tag": "markdown", "content": f"ğŸ”„ **æœ¬æ¬¡å…±æœ‰ {update_count} ä¸ªå·²çŸ¥é¡¹ç›®æ›´æ–°äº†æ•°æ®**"},
                    {"tag": "markdown", "content" : "æ‰‹åŠ¨@ZHYï¼Œè®°å¾—æ›´æ–°ä¸€ä¸‹å¤šç»´è¡¨æ ¼å“ˆï½"},
                    {"tag": "markdown", "content": f"ğŸ“ **æ›´æ–°æ‘˜è¦ï¼š**\n{log_preview}"},
                    {"tag": "hr"},
                    {"tag": "markdown", "content": f"<font color='grey' size='small'>ğŸ“Š ç´¯è®¡ç›‘æ§ï¼šä¸»æµ {total_major} | éä¸»æµ {total_other}\nğŸ“… ç›‘æ§æ—¶åˆ»ï¼š{get_now_bj()}</font>"}
                ]
            },
            "header": {
                "template": "red" if total_new > 0 else "blue",
                "title": {"content": f"GitHub ç›‘æ§ï¼šå‘ç° {total_new} ä¸ªæ–°é¡¹ç›®ï¼", "tag": "plain_text"},
                "icon": {"tag": "standard_icon", "token": "code_outlined"}
            },
            "schema": "2.0"
        }
    }
    try:
        res = requests.post(FEISHU_WEBHOOK, json=card_payload, timeout=10)
        print(f"âœ… [é£ä¹¦] æ¨é€å®Œæˆï¼Œå“åº”çŠ¶æ€: {res.status_code}")
    except Exception as e:
        print(f"âŒ [é£ä¹¦] æ¨é€å¤±è´¥: {e}")

# ================= 6. ä¸»ç¨‹åºè¿è¡Œå…¥å£ =================

def main():
    print(f"ğŸ“… --- ç›‘æ§ä»»åŠ¡å¯åŠ¨æ—¶åˆ»: {get_now_bj()} ---")
    if not TOKEN:
        print("âŒ é”™è¯¯: GITHUB_TOKEN æœªé…ç½®")
        return

    print("ğŸ›°ï¸ [2/6] æ­£åœ¨æ‰«æ GitHub...")
    spec_data, other_data = {}, {}
    tasks = {f"license:{lic}": "SPEC" for lic in SPECIFIC_LICENSES}
    tasks[" ".join([f"-license:{lic}" for lic in SPECIFIC_LICENSES])] = "OTHER"

    with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:
        f_to_q = {executor.submit(fetch_github_data, q): group for q, group in tasks.items()}
        for f in concurrent.futures.as_completed(f_to_q):
            group = f_to_q[f]
            for item in f.result():
                (spec_data if group == "SPEC" else other_data)[item['id']] = item

    print("ğŸ“Š [3/6] å¼€å§‹å¤„ç†å¢é‡ä¸æŒ‡æ ‡åˆ†æ...")
    new_spec, upd_spec, tot_spec, logs_spec = process_incremental(list(spec_data.values()), MAJOR_TOTAL_CSV, "Major")
    new_other, upd_other, tot_other, logs_other = process_incremental(list(other_data.values()), OTHER_TOTAL_CSV, "Other")

    print("ğŸ¤– [4/6] å‡†å¤‡è§¦å‘ Coze å·¥ä½œæµ...")
    all_new_items = new_spec + new_other
    coze_status = run_coze_workflow(all_new_items)

    print("ğŸ“ [5/6] è®°å½•æœ¬åœ°æ“ä½œæ—¥å¿—...")
    all_logs = logs_spec + logs_other
    if all_logs:
        with open(LOG_FILE, "a", encoding="utf-8") as f:
            f.write("\n".join([l for l in all_logs if l.strip()]) + f"\n--- {get_now_bj()} ---\n\n")

    print("âœ‰ï¸ [6/6] å‘é€é£ä¹¦æ—¥æŠ¥å¡ç‰‡...")
    send_feishu_v2_card(new_spec, new_other, upd_spec + upd_other, tot_spec, tot_other, all_logs, coze_success=coze_status)
    
    print(f"âœ¨ ç›‘æ§ä»»åŠ¡é¡ºåˆ©ç»“æŸï¼[æ—¶åˆ»: {get_now_bj()}]")

if __name__ == "__main__":
    main()
