import requests
import concurrent.futures
import os
import pandas as pd
from datetime import datetime, timezone, timedelta
from dotenv import load_dotenv

from cozepy import Coze, TokenAuth, COZE_CN_BASE_URL


# ================= 1. é…ç½®ä¸åˆå§‹åŒ– =================

print("ğŸš€ åˆå§‹åŒ–ç¯å¢ƒ...")

if os.path.exists(".env"):
    load_dotenv()

TOKEN = os.getenv("GITHUB_TOKEN")
FEISHU_WEBHOOK = os.getenv("FEISHU_WEBHOOK")
COZE_API_TOKEN = os.getenv("COZE_API_TOKEN")

workflow_id = "7600384889276547126"

BASE_QUERY = "skills language:Python created:>2025-10-10 is:public stars:>100"
SPECIFIC_LICENSES = ["mit", "apache-2.0", "gpl-3.0", "0bsd", "cc0-1.0"]

DIR_TOTAL, DIR_CHANGES, DIR_LOGS = "Data_Total", "Data_Changes", "Logs"
MAJOR_TOTAL_CSV = os.path.join(DIR_TOTAL, "major_licenses_total.csv")
OTHER_TOTAL_CSV = os.path.join(DIR_TOTAL, "other_licenses_total.csv")
LOG_FILE = os.path.join(DIR_LOGS, "update_log.txt")

for d in [DIR_TOTAL, DIR_CHANGES, DIR_LOGS]:
    os.makedirs(d, exist_ok=True)


# ================= 2. å·¥å…·å‡½æ•° =================

def get_now_bj():
    return datetime.now(timezone.utc).astimezone(
        timezone(timedelta(hours=8))
    ).strftime("%Y-%m-%d %H:%M:%S")


def convert_to_bj_time(utc_str):
    if not utc_str:
        return ""
    try:
        dt = datetime.strptime(
            utc_str, "%Y-%m-%dT%H:%M:%SZ"
        ).replace(tzinfo=timezone.utc)
        return dt.astimezone(
            timezone(timedelta(hours=8))
        ).strftime("%Y-%m-%d %H:%M:%S")
    except Exception:
        return utc_str


def save_daily_change(df, prefix, label, date_suffix):
    path = os.path.join(DIR_CHANGES, f"{prefix}_{label}_{date_suffix}.csv")
    if os.path.exists(path):
        old = pd.read_csv(path)
        df = pd.concat([old, df]).drop_duplicates("Repo_ID", keep="last")
    df.to_csv(path, index=False, encoding="utf-8-sig")
    print(f"    ğŸ’¾ ä¿å­˜æ–‡ä»¶ {path}")


# ================= 3. GitHub æŠ“å– =================

def fetch_github_data(query_suffix):
    url = "https://api.github.com/search/repositories"
    headers = {
        "Accept": "application/vnd.github+json",
        "User-Agent": "Repo-Monitor-Bot"
    }
    if TOKEN:
        headers["Authorization"] = f"Bearer {TOKEN}"

    try:
        res = requests.get(
            url,
            headers=headers,
            params={
                "q": f"{BASE_QUERY} {query_suffix}",
                "sort": "stars",
                "order": "desc",
                "per_page": 100
            },
            timeout=20
        )
        if res.status_code == 200:
            items = res.json().get("items", [])
            print(f"    - [{query_suffix}] {len(items)} æ¡")
            return items
    except Exception as e:
        print("    - æŸ¥è¯¢å¼‚å¸¸:", e)

    return []


# ================= 4. Cozeï¼ˆé€»è¾‘ä¸å˜ï¼Œå ä½ï¼‰ =================

def run_coze_workflow(new_items):
    if not COZE_API_TOKEN or not new_items:
        return True
    try:
        print("ğŸ¤– Coze workflow å·²è§¦å‘ï¼ˆå ä½ï¼‰")
        return True
    except Exception:
        return False


# ================= 5. å¢é‡å¤„ç† =================

def process_incremental(items, file_path, label):
    now = get_now_bj()
    date_suffix = datetime.now().strftime("%m%d")

    new_df = pd.DataFrame([
        {
            "Repo_ID": i["id"],
            "Name": i["full_name"],
            "Stars": i["stargazers_count"],
            "License": i["license"]["key"] if i["license"] else "None",
            "URL": i["html_url"],
            "Created_At": convert_to_bj_time(i["created_at"]),
            "Updated_At": convert_to_bj_time(i["updated_at"]),
            "Last_Grabbed_At": now
        }
        for i in items
    ])

    if not os.path.exists(file_path):
        new_df["First_Grabbed_At"] = now
        new_df.to_csv(file_path, index=False, encoding="utf-8-sig")
        return [], 0, len(new_df), []

    old_df = pd.read_csv(file_path)
    old_df["Repo_ID"] = old_df["Repo_ID"].astype(int)

    # æ–°å¢
    new_mask = ~new_df["Repo_ID"].isin(old_df["Repo_ID"])
    new_items_df = new_df[new_mask].copy()
    if not new_items_df.empty:
        new_items_df["First_Grabbed_At"] = now
        save_daily_change(new_items_df, "New", label, date_suffix)

    # æ›´æ–°
    merged = pd.merge(new_df, old_df, on="Repo_ID", suffixes=("_new", "_old"))
    changed = merged[
        (merged["Stars_new"] != merged["Stars_old"]) |
        (merged["Updated_At_new"] != merged["Updated_At_old"])
    ]
    if not changed.empty:
        changed_df = new_df[new_df["Repo_ID"].isin(changed["Repo_ID"])]
        save_daily_change(changed_df, "Update", label, date_suffix)

    first_map = old_df.set_index("Repo_ID")["First_Grabbed_At"].to_dict()
    new_df["First_Grabbed_At"] = new_df["Repo_ID"].map(first_map).fillna(now)

    total_df = pd.concat([new_df, old_df]).drop_duplicates("Repo_ID", keep="first")
    total_df.to_csv(file_path, index=False, encoding="utf-8-sig")

    logs = [
        f"[{label}] æ–°å¢ï¼š{r['Name']} (â˜…{r['Stars']})"
        for _, r in new_items_df.iterrows()
    ]

    return (
        new_items_df.to_dict("records"),
        len(changed),
        len(total_df),
        logs
    )


# ================= 6. âš ï¸ åŸå°ä¸åŠ¨çš„é£ä¹¦å¡ç‰‡æ„å»º =================

def build_feishu_v2_card(
    new_major, new_other, update_count,
    total_major, total_other, all_logs, coze_success=True
):
    total_new = len(new_major) + len(new_other)

    major_md = "\n".join(
        [f"â€¢ [{i['Name']}]({i['URL']}) <font color='grey'>ğŸ£{i['Created_At'][:10]}</font> **â˜… {i['Stars']}**"
         for i in new_major[:5]]
    ) or "æš‚æ— æ–°å¢"

    other_md = "\n".join(
        [f"â€¢ [{i['Name']}]({i['URL']}) <font color='grey'>ğŸ£{i['Created_At'][:10]}</font> **â˜… {i['Stars']}**"
         for i in new_other[:5]]
    ) or "æš‚æ— æ–°å¢"

    log_preview = "\n".join([l for l in all_logs if l.strip()][:8])

    card_payload = {
        "msg_type": "interactive",
        "card": {
            "body": {
                "direction": "vertical",
                "elements": [
                    {
                        "tag": "column_set",
                        "flex_mode": "stretch",
                        "horizontal_spacing": "12px",
                        "columns": [
                            {
                                "tag": "column",
                                "width": "weighted",
                                "weight": 1,
                                "background_style": "red-50",
                                "padding": "12px",
                                "elements": [
                                    {"tag": "markdown", "content": "**<font color='red'>ä¸»æµç»„</font>**"},
                                    {"tag": "markdown", "content": major_md}
                                ]
                            },
                            {
                                "tag": "column",
                                "width": "weighted",
                                "weight": 1,
                                "background_style": "orange-50",
                                "padding": "12px",
                                "elements": [
                                    {"tag": "markdown", "content": "**<font color='orange'>éä¸»æµç»„</font>**"},
                                    {"tag": "markdown", "content": other_md}
                                ]
                            }
                        ]
                    },
                    {"tag": "markdown", "content": f"ğŸ”„ **æœ¬æ¬¡å…±æœ‰ {update_count} ä¸ªå·²çŸ¥é¡¹ç›®æ›´æ–°äº†æ•°æ®**"},
                    {"tag": "markdown", "content": f"ğŸ“ **æ›´æ–°æ‘˜è¦ï¼š**\n{log_preview}"},
                    {"tag": "hr"},
                    {
                        "tag": "markdown",
                        "content": (
                            f"<font color='grey' size='small'>"
                            f"ğŸ“Š ç´¯è®¡ç›‘æ§ï¼šä¸»æµ {total_major} | éä¸»æµ {total_other}\n"
                            f"ğŸ“… ç›‘æ§æ—¶åˆ»ï¼š{get_now_bj()}"
                            f"</font>"
                        )
                    },
                    {
                        "behaviors": [
                            {
                                "default_url": "https://bytedance.larkoffice.com/base/ObLQbDL5QaWfypsafgecLuhRn8f?from=from_copylink",
                                "type": "open_url"
                            }
                        ],
                        "element_id": "custom_id",
                        "margin": "4px 0px 4px 0px",
                        "tag": "button",
                        "text": {"content": "å·²åŒæ­¥è‡³å¤šç»´è¡¨æ ¼ ç‚¹å‡»æŸ¥çœ‹", "tag": "plain_text"},
                        "type": "primary_filled",
                        "width": "fill"
                    }
                ]
            },
            "header": {
                "template": "red" if total_new > 0 else "blue",
                "title": {
                    "content": f"GitHub ç›‘æ§ï¼šå‘ç° {total_new} ä¸ªæ–°é¡¹ç›®ï¼",
                    "tag": "plain_text"
                },
                "icon": {"tag": "standard_icon", "token": "code_outlined"}
            },
            "schema": "2.0"
        }
    }

    return card_payload


# ================= 7. ç»Ÿä¸€ Webhook å‘é€ =================

def send_feishu_webhook(payload):
    if not FEISHU_WEBHOOK:
        print("âš ï¸ æœªé…ç½®é£ä¹¦ Webhook")
        return
    res = requests.post(FEISHU_WEBHOOK, json=payload, timeout=10)
    print(f"âœ… Webhook æ¨é€å®Œæˆ: {res.status_code}")


# ================= 8. ä¸»ç¨‹åº =================

def main():
    print(f"ğŸ“… å¯åŠ¨æ—¶é—´ {get_now_bj()}")

    spec_data, other_data = {}, {}

    tasks = {f"license:{l}": "SPEC" for l in SPECIFIC_LICENSES}
    tasks[" ".join(f"-license:{l}" for l in SPECIFIC_LICENSES)] = "OTHER"

    with concurrent.futures.ThreadPoolExecutor(max_workers=6) as ex:
        fs = {ex.submit(fetch_github_data, q): g for q, g in tasks.items()}
        for f in concurrent.futures.as_completed(fs):
            for item in f.result():
                (spec_data if fs[f] == "SPEC" else other_data)[item["id"]] = item

    new_spec, upd_spec, tot_spec, logs_spec = process_incremental(
        list(spec_data.values()), MAJOR_TOTAL_CSV, "Major"
    )
    new_other, upd_other, tot_other, logs_other = process_incremental(
        list(other_data.values()), OTHER_TOTAL_CSV, "Other"
    )

    coze_status = run_coze_workflow(new_spec + new_other)

    all_logs = logs_spec + logs_other
    if all_logs:
        with open(LOG_FILE, "a", encoding="utf-8") as f:
            f.write("\n".join(all_logs) + "\n\n")

    card = build_feishu_v2_card(
        new_spec, new_other,
        upd_spec + upd_other,
        tot_spec, tot_other,
        all_logs,
        coze_success=coze_status
    )

    payload = {
        "event": "github_repo_monitor",
        "timestamp": get_now_bj(),
        "data": {
            "new": {
                "major": new_spec,
                "other": new_other
            },
            "update_count": upd_spec + upd_other
        },
        "meta": {
            "total_major": tot_spec,
            "total_other": tot_other,
            "coze_status": coze_status
        },
        "card": card
    }

    send_feishu_webhook(payload)
    print("âœ¨ ä»»åŠ¡ç»“æŸ")


if __name__ == "__main__":
    main()
